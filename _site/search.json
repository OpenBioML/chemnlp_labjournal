[
  {
    "objectID": "posts/01/index.html",
    "href": "posts/01/index.html",
    "title": "Welcome and background",
    "section": "",
    "text": "This is the first post of the ChemNLP project under the umbrella of the OpenBioML collective research laboratory.\nOur aim is to (1) create a massive chemistry dataset and (2) train a large language model (LLM) on this dataset for downstream applications that include\nAll the outputs will be open source and freely accessible. Also all the work happens collaboratively, organized via Discord and GitHub."
  },
  {
    "objectID": "posts/01/index.html#background",
    "href": "posts/01/index.html#background",
    "title": "Welcome and background",
    "section": "Background",
    "text": "Background\n\nMachine learning in chemistry\nChemistry is probably one of the most exciting applications for machine learning. Chemistry gives us the tools to create drugs, chemistry can also help to fight climate change. (Yao et al. 2022; Tabor et al. 2018)\nHowever, chemists are also spoiled for choice. (Walsh 2015) There are too many things chemists could try and many experiments are costly (or even involve experiments). Any tool we could use to speed up the design of chemical compounds would hence be of tremendous value.\nInterestingly, chemistry also creates a lot of data - but most of the data is currently unused (or hidden in journal articles or lab notebooks). (Jablonka, Patiny, and Smit 2022)\nUsing the data that has been curated, various models have been built, often with pre-computed features (resulting in tabular datasets that are often consumed with models such as XGBoost), graph neural networks, or so-called line-representations (“linearizations” of the molecule graph) such as SMILES and SELFIES. (Krenn et al. 2022)\nUsing the latter, various groups have been training transformers to predict molecular properties, convert between molecular representations, (Edwards et al. 2022) or to even directly create molecules with desired properties.\n\n\nLLMs for chemistry\nLarge language models (Hu and Buehler 2023)\nSchwaller, MolT5, the PubChem article Galactica\n\nUsing models from the GPT-3 family\n(Hocky and White 2022)\n(Jablonka et al. 2023)\n(Frey et al. 2022)\n(Dunn et al. 2022)"
  },
  {
    "objectID": "posts/01/index.html#getting-involved",
    "href": "posts/01/index.html#getting-involved",
    "title": "Welcome and background",
    "section": "Getting involved",
    "text": "Getting involved\n\nJoin the discord community and sign up\nThe first step to get involved with the community is to join our Discord server, in the ChemNLP channel, we discuss the project. If you want to contribute, you can also sign up on this Google Sheet.\n\n\nComment on the proposal\nIf you want to get involved in shaping the project, you can comment on the proposal draft.\n\n\nSelect an issue\nIf you want to get your hands dirty with collecting and clearning code or writing pipelines, you can find a list of open issues on GitHub. Simply comment on the issue you would like to work. If you would like to work on something else, simple create an issue such that we can keep track of all the progress.\nIn particular, we are currently collecting open source datasets. If you know some datasets, you can add them to our Awesome List. (Or feel free to improve existing entries.)\nFor more details, check the contribution guide."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "In this website, we will publish regular progress updates of the ChemNLP project under the umbrella of the OpenBioML collective research laboratory."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ChemNLP",
    "section": "",
    "text": "ChemNLP progress update\n\n\n\n\n\n\n\n\n\n\n\nWelcome and background\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nFeb 15, 2023\n\n\nKevin M Jablonka, Michael Piechler, Andrew White\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/01/index.html#the-chemnlp-project",
    "href": "posts/01/index.html#the-chemnlp-project",
    "title": "Welcome and background",
    "section": "The ChemNLP project",
    "text": "The ChemNLP project\nInspired by the transformational potential of large language model we aim to create a massive chemistry dataset to then train a foundational model focussed on applications in chemistry."
  },
  {
    "objectID": "posts/01/index.html#references",
    "href": "posts/01/index.html#references",
    "title": "Welcome and background",
    "section": "References",
    "text": "References\n\n\nDunn, Alexander, John Dagdelen, Nicholas Walker, Sanghoon Lee, Andrew S. Rosen, Gerbrand Ceder, Kristin Persson, and Anubhav Jain. 2022. “Structured Information Extraction from Complex Scientific Text with Fine-Tuned Large Language Models.” arXiv Preprint arXiv: Arxiv-2212.05238.\n\n\nEdwards, Carl, Tuan Lai, Kevin Ros, Garrett Honke, Kyunghyun Cho, and Heng Ji. 2022. “Translation Between Molecules and Natural Language.” arXiv. https://doi.org/10.48550/ARXIV.2204.11817.\n\n\nFrey, Nathan, Ryan Soklaski, Simon Axelrod, Siddharth Samsi, Rafael Gomez-Bombarelli, Connor Coley, and Vijay Gadepally. 2022. “Neural Scaling of Deep Chemical Models,” May. https://doi.org/10.26434/chemrxiv-2022-3s512.\n\n\nHocky, Glen M., and Andrew D. White. 2022. “Natural Language Processing Models That Automate Programming Will Transform Chemistry Research and Teaching.” Digital Discovery 1 (2): 79–83. https://doi.org/10.1039/d1dd00009h.\n\n\nHu, Yiwen, and Markus J. Buehler. 2023. “Deep Language Models for Interpretative and Predictive Materials Science.” APL Machine Learning 1 (1): 010901. https://doi.org/10.1063/5.0134317.\n\n\nJablonka, Kevin Maik, Luc Patiny, and Berend Smit. 2022. “Making the Collective Knowledge of Chemistry Open and Machine Actionable.” Nat. Chem. 14 (4): 365–76. https://doi.org/10.1038/s41557-022-00910-7.\n\n\nJablonka, Kevin Maik, Philippe Schwaller, Andres Ortega-Guerrero, and Berend Smit. 2023. “Is GPT-3 All You Need for Low-Data Discovery in Chemistry?” February. https://doi.org/10.26434/chemrxiv-2023-fw8n4.\n\n\nKrenn, Mario, Qianxiang Ai, Senja Barthel, Nessa Carson, Angelo Frei, Nathan C. Frey, Pascal Friederich, et al. 2022. “SELFIES and the Future of Molecular String Representations.” Patterns 3 (10): 100588. https://doi.org/10.1016/j.patter.2022.100588.\n\n\nTabor, Daniel P., Loïc M. Roch, Semion K. Saikin, Christoph Kreisbeck, Dennis Sheberla, Joseph H. Montoya, Shyam Dwaraknath, et al. 2018. “Accelerating the Discovery of Materials for Clean Energy in the Era of Smart Automation.” Nat Rev Mater 3 (5): 5–20. https://doi.org/10.1038/s41578-018-0005-z.\n\n\nWalsh, Aron. 2015. “The Quest for New Functionality.” Nature Chem 7 (4): 274–75. https://doi.org/10.1038/nchem.2213.\n\n\nYao, Zhenpeng, Yanwei Lum, Andrew Johnston, Luis Martin Mejia-Mendoza, Xin Zhou, Yonggang Wen, Alán Aspuru-Guzik, Edward H. Sargent, and Zhi Wei Seh. 2022. “Machine Learning for a Sustainable Energy Future.” Nat Rev Mater, October. https://doi.org/10.1038/s41578-022-00490-5."
  },
  {
    "objectID": "posts/01/index.html#progress-so-far",
    "href": "posts/01/index.html#progress-so-far",
    "title": "Welcome and background",
    "section": "Progress so far",
    "text": "Progress so far\nAlready before our first meeting we made some progress:\n\nwe started curating chemistry datasets in our Awesome List\nwe compiled a dataset from ChemRxiv (thanks to (marianna13?))"
  }
]