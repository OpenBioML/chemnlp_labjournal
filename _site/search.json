[
  {
    "objectID": "posts/01/index.html",
    "href": "posts/01/index.html",
    "title": "Welcome and background",
    "section": "",
    "text": "This is the first post of the ChemNLP project under the umbrella of the OpenBioML collective research laboratory.\nOur aim is to (1) create a massive chemistry dataset and (2) train a large language model (LLM) on this dataset for downstream applications that include\nAll the outputs will be open source and freely accessible. Also all the work happens collaboratively, organized via Discord and GitHub."
  },
  {
    "objectID": "posts/01/index.html#background",
    "href": "posts/01/index.html#background",
    "title": "Welcome and background",
    "section": "Background",
    "text": "Background\n\nMachine learning in chemistry\nChemistry is probably one of the most exciting applications for machine learning. Chemistry gives us the tools to create drugs, chemistry can also help to fight climate change. (Yao et al. 2022; Tabor et al. 2018)\nHowever, chemists are also spoiled for choice. (Walsh 2015) There are too many things chemists could try and many experiments are costly (or even involve experiments). Any tool we could use to speed up the design of chemical compounds would hence be of tremendous value.\nInterestingly, chemistry also creates a lot of data - but most of the data is currently unused (or hidden in journal articles or lab notebooks). (Jablonka, Patiny, and Smit 2022)\nUsing the data that has been curated, various models have been built, often with pre-computed features (resulting in tabular datasets that are often consumed with models such as XGBoost), graph neural networks, or so-called line-representations (“linearizations” of the molecule graph) such as SMILES and SELFIES. (Krenn et al. 2022)\nUsing the latter, various groups have been training transformers to predict molecular properties, convert between molecular representations, (Edwards et al. 2022) or to even directly create molecules with desired properties.\n\n\nLLMs for chemistry\n(Large) language models have found widespread use in chemistry. One of the most successful applications has been in the predicting reaction outcomes and retrosynthesis. (Schwaller et al. 2019) Around the same time, a team showed that word embeddings from models trained in unsupervised fashion on abstracts of papers can be used to discover new materials (Tshitoyan et al. 2019).\nIn parallel, transformer models have been pretrained for various applications in chemistry in materials science. For instance (Chithrananda, Grand, and Ramsundar 2020) pretrained RoBERTa 77 million compounds from the PubChem database (see also the prior works (Honda, Shi, and Ueda 2019; Maziarka et al. 2020)) and then fine-tuned it for classification tasks from MoleculeNet (Wu et al. 2018) where they also compared SMILES and SELFIES representation and BPE tokenization vs. a specific Regex-based tokenizer. The pretraining was recently scaled up by (Ross et al. 2022) (using masked language modeling) to 1.1 billion molecules (Molformer), who found that the fine-tuned model could outperform GNNs on some benchmark tasks.\n\n\nUsing models from the GPT-3 family\nVery recently, there has been a lot of interest in directly using pretrained models such as the ones from the GPT-3 for applications in chemistry.\n(Hocky and White 2022)\n(Jablonka et al. 2023)\n(Frey et al. 2022)\n(Dunn et al. 2022)"
  },
  {
    "objectID": "posts/01/index.html#getting-involved",
    "href": "posts/01/index.html#getting-involved",
    "title": "Welcome and background",
    "section": "Getting involved",
    "text": "Getting involved\n\nJoin the discord community and sign up\nThe first step to get involved with the community is to join our Discord server, in the ChemNLP channel, we discuss the project. If you want to contribute, you can also sign up on this Google Sheet.\n\n\nComment on the proposal\nIf you want to get involved in shaping the project, you can comment on the proposal draft.\n\n\nSelect an issue\nIf you want to get your hands dirty with collecting and clearning code or writing pipelines, you can find a list of open issues on GitHub. Simply comment on the issue you would like to work. If you would like to work on something else, simple create an issue such that we can keep track of all the progress.\nIn particular, we are currently collecting open source datasets. If you know some datasets, you can add them to our Awesome List. (Or feel free to improve existing entries.)\nFor more details, check the contribution guide."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "In this website, we will publish regular progress updates of the ChemNLP project under the umbrella of the OpenBioML collective research laboratory."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ChemNLP",
    "section": "",
    "text": "ChemNLP progress update\n\n\n\n\n\n\n\n\n\n\n\nWelcome and background\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nFeb 15, 2023\n\n\nKevin M Jablonka, Michael Piechler, Andrew White\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/01/index.html#the-chemnlp-project",
    "href": "posts/01/index.html#the-chemnlp-project",
    "title": "Welcome and background",
    "section": "The ChemNLP project",
    "text": "The ChemNLP project\nInspired by the transformational potential of large language model we aim to create a massive chemistry dataset to then train a foundational model focussed on applications in chemistry."
  },
  {
    "objectID": "posts/01/index.html#references",
    "href": "posts/01/index.html#references",
    "title": "Welcome and background",
    "section": "References",
    "text": "References\n\n\nChithrananda, Seyone, Gabriel Grand, and Bharath Ramsundar. 2020. “ChemBERTa: Large-Scale Self-Supervised Pretraining for Molecular Property Prediction.” arXiv Preprint arXiv: Arxiv-2010.09885.\n\n\nDunn, Alexander, John Dagdelen, Nicholas Walker, Sanghoon Lee, Andrew S. Rosen, Gerbrand Ceder, Kristin Persson, and Anubhav Jain. 2022. “Structured Information Extraction from Complex Scientific Text with Fine-Tuned Large Language Models.” arXiv Preprint arXiv: Arxiv-2212.05238.\n\n\nEdwards, Carl, Tuan Lai, Kevin Ros, Garrett Honke, Kyunghyun Cho, and Heng Ji. 2022. “Translation Between Molecules and Natural Language.” arXiv. https://doi.org/10.48550/ARXIV.2204.11817.\n\n\nFrey, Nathan, Ryan Soklaski, Simon Axelrod, Siddharth Samsi, Rafael Gomez-Bombarelli, Connor Coley, and Vijay Gadepally. 2022. “Neural Scaling of Deep Chemical Models,” May. https://doi.org/10.26434/chemrxiv-2022-3s512.\n\n\nHocky, Glen M., and Andrew D. White. 2022. “Natural Language Processing Models That Automate Programming Will Transform Chemistry Research and Teaching.” Digital Discovery 1 (2): 79–83. https://doi.org/10.1039/d1dd00009h.\n\n\nHonda, Shion, Shoi Shi, and Hiroki R. Ueda. 2019. “SMILES Transformer: Pre-Trained Molecular Fingerprint for Low Data Drug Discovery.” arXiv Preprint arXiv: Arxiv-1911.04738.\n\n\nJablonka, Kevin Maik, Luc Patiny, and Berend Smit. 2022. “Making the Collective Knowledge of Chemistry Open and Machine Actionable.” Nat. Chem. 14 (4): 365–76. https://doi.org/10.1038/s41557-022-00910-7.\n\n\nJablonka, Kevin Maik, Philippe Schwaller, Andres Ortega-Guerrero, and Berend Smit. 2023. “Is GPT-3 All You Need for Low-Data Discovery in Chemistry?” February. https://doi.org/10.26434/chemrxiv-2023-fw8n4.\n\n\nKrenn, Mario, Qianxiang Ai, Senja Barthel, Nessa Carson, Angelo Frei, Nathan C. Frey, Pascal Friederich, et al. 2022. “SELFIES and the Future of Molecular String Representations.” Patterns 3 (10): 100588. https://doi.org/10.1016/j.patter.2022.100588.\n\n\nMaziarka, Łukasz, Tomasz Danel, Sławomir Mucha, Krzysztof Rataj, Jacek Tabor, and Stanisław Jastrzębski. 2020. “Molecule Attention Transformer.” arXiv Preprint arXiv: Arxiv-2002.08264.\n\n\nRoss, Jerret, Brian Belgodere, Vijil Chenthamarakshan, Inkit Padhi, Youssef Mroueh, and Payel Das. 2022. “Large-Scale Chemical Language Representations Capture Molecular Structure and Properties.” Nat Mach Intell 4 (12): 1256–64. https://doi.org/10.1038/s42256-022-00580-7.\n\n\nSchwaller, Philippe, Teodoro Laino, Théophile Gaudin, Peter Bolgar, Christopher A. Hunter, Costas Bekas, and Alpha A. Lee. 2019. “Molecular Transformer: A Model for Uncertainty-Calibrated Chemical Reaction Prediction.” ACS Cent. Sci. Central Science 5 (9): 1572–83. https://doi.org/10.1021/acscentsci.9b00576.\n\n\nTabor, Daniel P., Loïc M. Roch, Semion K. Saikin, Christoph Kreisbeck, Dennis Sheberla, Joseph H. Montoya, Shyam Dwaraknath, et al. 2018. “Accelerating the Discovery of Materials for Clean Energy in the Era of Smart Automation.” Nat Rev Mater 3 (5): 5–20. https://doi.org/10.1038/s41578-018-0005-z.\n\n\nTshitoyan, Vahe, John Dagdelen, Leigh Weston, Alexander Dunn, Ziqin Rong, Olga Kononova, Kristin A. Persson, Gerbrand Ceder, and Anubhav Jain. 2019. “Unsupervised Word Embeddings Capture Latent Knowledge from Materials Science Literature.” Nature 571 (7763): 95–98. https://doi.org/10.1038/s41586-019-1335-8.\n\n\nWalsh, Aron. 2015. “The Quest for New Functionality.” Nature Chem 7 (4): 274–75. https://doi.org/10.1038/nchem.2213.\n\n\nWu, Zhenqin, Bharath Ramsundar, Evan N. Feinberg, Joseph Gomes, Caleb Geniesse, Aneesh S. Pappu, Karl Leswing, and Vijay Pande. 2018. “MoleculeNet: A Benchmark for Molecular Machine Learning.” Chem. Sci. 9 (2): 513–30. https://doi.org/10.1039/c7sc02664a.\n\n\nYao, Zhenpeng, Yanwei Lum, Andrew Johnston, Luis Martin Mejia-Mendoza, Xin Zhou, Yonggang Wen, Alán Aspuru-Guzik, Edward H. Sargent, and Zhi Wei Seh. 2022. “Machine Learning for a Sustainable Energy Future.” Nat Rev Mater, October. https://doi.org/10.1038/s41578-022-00490-5."
  },
  {
    "objectID": "posts/01/index.html#progress-so-far",
    "href": "posts/01/index.html#progress-so-far",
    "title": "Welcome and background",
    "section": "Progress so far",
    "text": "Progress so far\nAlready before our first meeting we made some progress:\n\nwe started curating chemistry datasets in our Awesome List\nwe compiled a dataset from ChemRxiv (thanks to (marianna13?))"
  }
]